import io
import re
import sys
#import time
import os
import posixpath
import json
import shutil
import traceback
import glob
#from datetime import datetime, timezone

import boto3
import numpy as np          # only to get np.nan
import pandas as pd
from botocore.exceptions import ClientError
#from cv2 import imwrite, imread, imencode, imdecode, IMREAD_GRAYSCALE
import cv2

from utilities import utils, args, logs
from aws_lambda import s3utils
#from utilities import config_d
#from models.Job import Job



class DB:
    """
    Abstraction layer working like a driver to control all data related
    actions like writing and reading.
    It's methods relate on `MODE` which is saved in env variable
    `STORE_TYPE` working like a switch.
    I.e. `MODE = 'local'` will use file related methods to work with
    the data like JSON files or local images.
    """
    MODE = 'local'
    BALLOT_MARKS_DF = pd.DataFrame()
    
    @classmethod
    def set_DB_mode(cls):
        cls.MODE = 's3' if args.argsdict['use_s3_results'] or utils.on_lambda() else 'local'


    @staticmethod
    def dirpath_from_dirname(
        dirname,                # primary dirname
        s3flag:bool = None,     # None: Follow DB.MODE, True: force s3, False: force local
        subdir:str = None,      # None: no subdir; Otherwise, can have multiple parts, divided by '/'
        local_dirname='EIFs'    # used only for 'config' directory and local mode to support 
                                #       repo relative paths.
        ):
        """ 
        for the most part, files are located directly in the dirname provided.
        including chunks from lambdas, and the resulting combination. 
        
        There is also the election-data buckets where the ballot imaage archives, and CVR files etc.
        
        election_data:
        argsdict['archives_folder_path'] or 
        argsdict['archives_folder_s3path']

        
        
        
        
        The following are on the job_bucket
        
        {job_name}/{archive_root}.zip                               # zip files generated by election officials on windows.
                  /{archive_root2}.zip
        
        
        sys_config/arg_specs.csv
                  /action_specs.csv
                  /upload_specs.csv
                  /config_specs.csv
                  /config_settings.csv
                  /upload_settings.csv
        
        
        config/JOB_{election_name}.csv
              /EIF_{election_name}.csv
              /BOF_{election_name}.csv
              /Manual_Styles_to_Contests_table.csv
        
        
        bif
           /{archive_root}_bif.csv                                  # resulting combined bif files, one per archive. 
                                                                    #   NOTE: only available if bif was generated by reading ballots
           /bif.csv                                                 # combined bif file
           /log_bif.txt                                             # combined logs for all archives.
           /exc_bif.txt                                             # combined exception report for all archives.
           /chunks/{archive_root}_bif_chunk_{chunk_idx}.csv         # individual bif chunks with {200} ballots per chunk or so.
           /chunks/log_{archive_root}_bif_chunk_{chunk_idx}.txt     #       individual bif_log for each chunk
           /chunks/exc_{archive_root}_bif_chunk_{chunk_idx}.txt     #       individual bif_exceptions for each chunk
                                                                    #   chunk_name = {archive_root}_bif_chunk_{chunk_idx}
                                                                    #   chunk_pat = fr"{archive_root}_bif_chunk_\d+"
        
        styles/contests_dod.json                                    # data from EIF after parsing.
                                                                    #  provides the text expected, and options for each contest.
        styles/CVR_STYLE_TO_CONTESTS_DICT.json                      # style to contests derived from CVR or BIF (dol)
        styles/CONV_card_code_TO_ballot_type_id_DICT.json           # This may be generated from CVR if available (Dominion only)
        styles/BUILT_BALLOTID_TO_STYLE_DICT.json                    # DEPRECATED - use BIF table instead.
        styles/sheetstyle_map_dict.json                             # maps similar styles
        styles/template_tasklists_dolod.json                        # single template tasklists file
        styles/tasks/{style_num}.csv                                # otherwise, a separate file is generated for each task here.
        styles/{style_num}/{style_num}-template{page1}.png          # up to two template files. A combination of {threshold} ballots. usually 50 right now. This is the result of gentemplate step.
                          /{style_num}-redlined{page1}.png          # redline proofs
                          /{style_num}-map_report.txt               # map report for this style.
                          /template_parts/{ballot_id}-{page1}.png   # up to {threshold} ballots used to create template (if set to save)
                          /{style_num}_style.json                   # style data, incl. timing marks. Rename {style_num}_style.json
                          /{style_num}_rois.json                    # rois list provides location and ocr_text for each rois -- result of genrois
        styles/{style_num}/rois_parts/{style_num}-{part_name}.png   # image processing steps + individual rois images (if set to save)
        
        styles/logs/log_{style_num}.txt                             # combined log for each style, including gentempate, genrois, maprois steps.
        styles/logs/exc_{style_num}.txt                             # combined exceptions for each style, including gentempate, genrois, maprois steps.
        styles/logs/map_report_{style_num}.txt                      # map_report for each style, including gentempate, genrois, maprois steps.
        styles/roismap/{style_num}-roismap.csv                      # map of contests to rois for each style.
        styles/style_map.csv                                        # (Proposed) map table, columns card_code, style_num, eff_style_num
        styles/map_report.txt                                       # combined map report
        styles/log_styles.txt                                       # combined logs
        styles/exc_styles.txt                                       # combined exceptions
        styles/proofs/{style_num}-redlined{page1}.png               # proofs are created only for effective styles.
        styles/{archive_root}_roismap.csv                           # roismap one per archive
        styles/roismap.csv                                          # combined roismap, used for extraction, reduced to effective styles.
                                                                    #   chunk_name = f"{style_num}"
                                                                    #   chunk_pat = fr"{style_num}"
                                                                    #   subdir = styles
                          
        exc_styles/{style_num}/<same structure as above>            # styles which did not properly build, copied.
                          
        exc_ballot/{ballot_id}/{ballot_id}.{tif|png|pdf}            # ballot exceptions: original ballot image(s)
                              /{ballot_id}-{name}.png               # diagnostic images
                              /{ballot_id}-exc-bif.txt              # exception report for this ballot.
                              /{ballot_id}-log-bif.txt              # log of processing for this ballot.
                              /{ballot_id}-exc-marks.txt            # exception report for this ballot.
                              /{ballot_id}-log-marks.txt            # log of processing for this ballot.
                              
        assist/{style}_{image files}                                # image files copied for processing by human assistant
              /{style}.json                                         # graphic annotation to images to assist in segmentation

        # current realization extracts ballots by archive.
        # once extracted, marks.csv files can be reorganized by
        # auditing chunk if it is more convenient to do it that
        # way. This information is in the BIF, i.e. which CVR 
        # the ballot is in might also be how the ballots are stored.
        # Or, a ballot manifest can be used to group the ballots
        # if they differ from the archives.

        marks/tasks/{archive_root}_chunk_{chunk_idx}.csv            # These are the extraction tasks, each is a group of bif records.
                                                                    # These are used again by cmpcvr and so must be saved to s3.
        marks/chunks/{archive_root}_chunk_{chunk_idx}.csv           # individual marks chunks. These are kept for cmpcvr
        marks/chunks/log_{archive_root}_chunk_{chunk_idx}.txt       # log of individual marks chunks.
        marks/chunks/exc_{archive_root}_chunk_{chunk_idx}.txt       # exceptions of individual marks chunks.
        marks/ballots/{ballotid}/{ballotid}-{part_name}.png         # images of each mark on selected ballots
        marks/{archive_root}_marks.csv                              # marks datafile, one per archive
        marks/marks.csv                                             # marks datafile, unified (this is used for high level results)
        marks/log_marks.txt                                         # combined log of vote extraction process
        marks/exc_marks.txt                                         # combined exceptions of vote extraction process
                                                                    #   chunk_name = f"{archive_root}_chunk_{chunk_idx}"
                                                                    #   chunk_pat = fr"{archive_root}_chunk_\d+"
                                                                    #   subdir = chunks
        
        # task lists from marks are used here as well.
        cmpcvr/{archive_root}_cmpcvr.csv                            # combined comparison chunks.
        cmpcvr/disagreed.csv                                        # combined disagreed comparison records
        cmpcvr/overvotes.csv
        cmpcvr/blank.csv
        cmpcvr/chunks/{archive_root}_chunk_{chunk_idx}.csv          # individual cmpcvr chunks
        cmpcvr/chunks/disagreed_{archive_root}_chunk_{chunk_idx}.csv    # individual cmpcvr disagreed chunks
        cmpcvr/chunks/overvotes_{archive_root}_chunk_{chunk_idx}.csv    # individual cmpcvr overvote chunks
        cmpcvr/chunks/log_{archive_root}_chunk_{chunk_idx}.txt      # log of individual cmpcvr chunks.
        cmpcvr/chunks/exc_{archive_root}_chunk_{chunk_idx}.txt      # exceptions of individual cmpcvr chunks.
        cmpcvr/log_cmpcvr.txt                                       # combined log of vote extraction process
        cmpcvr/exc_cmpcvr.txt                                       # combined exceptions of vote extraction process
                                                                    #   chunk_name = f"{archive_root}_chunk_{chunk_idx}"
                                                                    #   chunk_pat = fr"{archive_root}_chunk_\d+"
                                                                    #   subdir = chunks
        cmpcvr/cmpcvr.csv                                           # combined comparison.
        
        reports/exc_general.txt                                     # general exception report by non-lambda processes
               /bif_report.txt                                      # bif report.
               /map_report.txt
               /disagree_report.txt
               /results.txt
               /exc_report.txt                                      # overall exception report
               
        lambda_tracker/Completed/{status_file}                      # when a lambda completes its work, it will add status report to 
        lambda_tracker/Failed/{status_file}                         #   one of these two folders.
           
        """
        
        archives_dirname_list = [
            'archives',
            ]        
        
    
        job_dirname_list = [
            'config',           # input files uploaded from front end or per simulation
            
            'bif',              # create bif table chunks and combine in this folder.
            'styles',           # created base templates and other files related to styles. 
            'assist',           # templates that need human assistance to complete map
            'marks',            # chunks and resulting marks_df.csv, one per archive, as a result of the extraction
            'exc_styles',       # styles data for any styles that fail to map.
            'exc_ballot',       # ballot data for any ballots that fail to align, register, or barcode-read.
            'cmpcvr',           # comparison result, chunks and combined.
            'results',          # high level report generated from marks, and comparison result.
            'reports',          # combined log files, map_report, exception_reports
            'summary',          # not sure
            'logs',             # logs and exception reports chunks.
            'tmp',              # temporary local folder only used within single scope.            
            'lambda_tracker',   # folder for both completed and failed lambda reports.
            'template_tasks',   # only if individual template task files are used. Now creating a single file.
            'extraction_tasks', # if separate files are created for extraction they are placed here. Probably better to use combined file as lolod.
                                # or indexes into bif.
            ]
            
        sys_dirname_list = [
            'sys_config',       # system settings, not job settings or user settings.
            'EIFs',
            'input_files',
            ]
            
        s3 = False        
        if dirname in sys_dirname_list:
            base_dirpath = None
            
        
        elif dirname in job_dirname_list:
           
            if (s3flag != False) and (DB.MODE == 's3' or utils.on_lambda()):
                # when on lambda, all files will be on s3, but also can be forced to s3 if s3flag == True.
                # this happens during testing or when code is operating in EC2 and chunks are downloaded from s3 to be combined.
                # if s3flag == False, this will force local operation. 
                # if s3flag == None, then follows DB.MODE or utils.on_lambda().
                base_dirpath = args.argsdict['job_folder_s3path']
                s3 = True
            else: 
                base_dirpath = args.argsdict['job_folder_path']

            if not base_dirpath:
                utils.sts("job_folder_path and s3path not provided, fatal error.")
                traceback.print_stack()
                sys.exit(1)

        elif dirname in archives_dirname_list:

            if (s3flag != False) and (args.argsdict['use_s3_archives'] or utils.on_lambda()):
                # when on lambda, all files will be on s3, but also can be forced to s3 if s3flag == True.
                # generally, the load_data() function is used only for CVR and other files.
                base_dirpath = args.argsdict['archives_folder_s3path']
                s3 = True
            else: 
                base_dirpath = args.argsdict['archives_folder_path']

            if not base_dirpath:
                utils.sts("archives_folder_path and s3path not provided, fatal error.")
                traceback.print_stack()
                sys.exit(1)

            
        else:
            print (f"invalid dirname '{dirname}' passed to DB")
            traceback.print_stack()
            sys.exit(1)
                
        if base_dirpath is None:
            if subdir is not None:
                dirpath = os.path.join(dirname, subdir, '')   # force ending /  
            else:
                dirpath = os.path.join(dirname, '')   # force ending /
        else:    
            if subdir is not None:
                dirpath = os.path.join(base_dirpath, dirname, subdir, '')   # force ending /
            else:
                dirpath = os.path.join(base_dirpath, dirname, '')   # force ending /

                
        if not s3:
            os.makedirs(os.path.dirname(dirpath), exist_ok=True)
            dirpath = utils.path_sep_per_os(dirpath, sep='\\')   # correct separators based on os.
        else:
            dirpath = utils.path_sep_per_os(dirpath, sep='/')   # correct separators based on os.
            
        return dirpath


    @staticmethod
    def get_config_filepath(local_config_folder, filename):
    
        if not local_config_folder in ['EIFs']:
            print("Logic Error")
            sys.exit(1)
    
        if args.argsdict['use_s3_config']:
            dirpath = DB.dirpath_from_dirname('config', s3flag=True)
        else:
            dirpath = DB.dirpath_from_dirname(local_config_folder, s3flag=False)
        
        config_path = dirpath + filename
        return config_path



    """-------------------------------------------------------------------------------
        the following two functions serve as the common interface to save data.
        In this application, all data is saved as files, either locally or on s3.

        Whether data is saved locally or on s3 depends primarily on DB.MODE setting,
        which should be set during initialization depending on settings parameters:
            use_s3_results, use_lambda, and on_lambda(). This is established here
        through use of the dirpath_from_dirname() function.

        These functions can save/load various types of files based either on 
        the extension of the name, or the format parameter, if it is provided.
        These functions are appropriate for internal use only, and are not suited
        to process any files generated outside the application. 

            format/extension    operation
            ----------------    ---------------------------------------
                .json           data item encoded/decoded from JSON
                                appropriate for saving Python objects.
                .csv            dataframe encoded as csv, no comments, etc.
                                This is used vs. .json for all tabular data
                                so that they can be easily concatenated.
                .png            Image data sourced by the application.
            ----------------    ---------------------------------------

        parameters:
            dirname     short-form dirname classification of this data item
                        this determines the folder where the item is stored.
            subdir      subdirectory (optional) 
            name        name of the object. if an extension is provided, it
                        also determines the format.
            format      determines source/dest format if no extension is 
                        provided. Uses extension formation, like '.csv'
            type        usually type can be inferred from format. But
                        type is provided to read df from json
            s3flag      Will override automatic settings. Useful in operations
                        that include both local and s3 filessystems.
                        Otherwise, set to None to use existing settings.
            silent_error:   If true, data object may not exist and then None
                            is returned.
            user_format bool, if true, allows comments, blank lines from csv.
                        Only applicable to load_data.
    --------------------------------------------------------------------------
    """
    
    @staticmethod
    def load_data(dirname, name, format='.json', type=None, subdir=None, s3flag=None, silent_error=False, user_format=False, dtype=None):
        from aws_lambda import s3utils

        #--- build path with extension
        dirpath = DB.dirpath_from_dirname(dirname, subdir=subdir, s3flag=s3flag)
        file_path = f"{dirpath}{name}"
        
        extension = os.path.splitext(name)[1]
        
        if extension in ['.json', '.csv', '.png', '.txt', '.xlsx']:
            format = extension
            
        if format in ['.json', '.csv', '.png', '.txt', '.xlsx']:
            if not file_path.endswith(format):
                file_path += format
        else:
            print (f"Logic error, format {format} not supported.")
            traceback.print_stack()
            sys.exit(1)

        if type is None and format in ['.csv', '.xlsx']: type = 'df'
        if format in ['.json']: type = 'obj'
        if format in ['.txt']:  type = 'txt'
        if format in ['.png']:  type = 'image'

        if not type or not format:
            print(f"Logic error: DB.save_data, type={type}, format={format}")
            traceback.print_stack()
            sys.exit(1)

        #--- read file into buffer based on source
        print(f"Reading {file_path}")       # this cannot use utils.sts

        if file_path.startswith('s3'):
            if s3utils.does_s3path_exist(file_path):
                if format == '.xlsx' and type == 'df':
                    return s3utils.read_xlsx_from_s3path(file_path, user_format=user_format)
                elif format == '.csv' and type == 'df':
                    return s3utils.read_csv_from_s3path(file_path, user_format=user_format, dtype=dtype)
                elif format == '.csv' and type == 'lod':    
                    return s3utils.read_lod_from_s3path(s3path=file_path, user_format=user_format)
                elif format == '.json':
                    buff = s3utils.read_buff_from_s3path(file_path)
                    return json.loads(buff)
                elif format == '.txt':
                    return s3utils.read_buff_from_s3path(file_path)
                elif format == '.png':
                    buff = s3utils.read_buff_from_s3path(file_path)
                    img_array = np.asarray(bytearray(buff), dtype=np.uint8)
                    return cv2.imdecode(img_array, 0)
                else:
                    print(f"Logic error, {format} not supported: DB.load_data")
                    sys.exit(1)
                     
            elif silent_error:
                return None
            else:
                raise FileNotFoundError(f'File {file_path} not found.')
        else:
            file_path = utils.path_sep_per_os(file_path)
            if os.path.isfile(file_path):
                try:
                    if format == '.csv':
                        df = DB.read_local_csv_to_df(file_path=file_path, user_format=user_format, silent_error=silent_error, dtype=dtype)
                        if df is None: return df
                        if type == 'lod':
                            return df.to_dict(orient='records')
                        return df
                            
                    elif format == '.json':
                        with open(file_path, 'r') as file:
                            return json.load(file)
                    elif format == '.txt':    
                        with open(file_path, 'r') as file:
                            return file.read()
                    elif format == '.png':
                        return cv2.imread(file_path, 0)
                    else:
                        print(f"Logic error: format {format} not supported in DB.load_data")
                        sys.exit(1)
                except OSError:
                    if silent_error: 
                        return None
                    else:
                        print (f"could not open {file_path}")
                        traceback.print_stack()
                        sys.exit(1)
            elif silent_error: 
                return None
            else:
                print (f"could not open {file_path}")
                traceback.print_stack()
                sys.exit(1)

        #--- write buffer based format and type
        if type == 'df':
            if format == '.json':
                return pd.read_json(io.StringIO(str(buff)))
            elif format == '.csv':
                return pd.read_csv(io.StringIO(str(buff)), na_filter=False, index_col=False)
            # elif format == '.xlsx':
                # return pd.read_xlsx(buff)
            else:
                print(f"Logic error: format {format} not supported with type df")
                traceback.print_stack()
                sys.exit(1)
                
        elif type == 'obj' or type is None:
            if format == '.json':
                return json.loads(buff)
            else:
                print(f"Logic error: format {format} not supported with type obj")
                traceback.print_stack()
                sys.exit(1)


    @staticmethod
    def save_data(data_item, dirname, name, format='.json', type=None, subdir=None, s3flag=None) -> str:
        """ save data_item at dirname, subdir, name with format, type specified.
            returns pathname where file is saved
        """
    
        from aws_lambda import s3utils

        #--- build path with extension
        dirpath = DB.dirpath_from_dirname(dirname, subdir=subdir, s3flag=s3flag)
        file_path = f"{dirpath}{name}"
            
        extension = os.path.splitext(name)[1]
        
        if extension in ['.json', '.csv', '.png', '.pdf', '.txt']:
            format = extension
            
        if format in ['.json', '.csv', '.png', '.pdf', '.txt']:
            if not extension:
                file_path += format
        else:
            print (f"Logic error, format {format} not supported.")
            traceback.print_stack()
            sys.exit(1)
            
        if format == '.png':
            type = 'image'
            
        if format == '.pdf':
            type = 'binary'
            
        if type is None and format in ['.csv']:
            type = 'df'
            
        if format in ['.json']:
            type = 'obj'

        if format in ['.txt']:
            type = 'txt'

        if not type or not format:
            print(f"Logic error: DB.save_data, type={type}, format={format}")
            traceback.print_stack()
            sys.exit(1)
            
        #--- convert data item to buffer
        if format == '.csv' and type == 'lod':
            df_item = pd.DataFrame(data_item, index=None)
            buff = df_item.to_csv(None, index=False)

        elif type == 'df':
            if format == '.json':
                buff = data_item.to_json(None, index=False)
            elif format == '.csv':
                buff = data_item.to_csv(None, index=False)
            # elif format == '.xlsx':
                # buff = pd.to_xlsx(data_item, index=False)
            else:
                print(f"Logic error: format {format} not supported with type df")
                traceback.print_stack()
                sys.exit(1)
                
        elif type == 'obj' or type is None:
            if format == '.json':
                buff = json.dumps(data_item)
            else:
                print(f"Logic error: format {format} not supported with type obj")
                traceback.print_stack()
                sys.exit(1)
                
        elif type == 'image':
            buff = cv2.imencode(format, data_item)[1].tostring()
            
        elif type in ['binary', 'txt']:
            buff = data_item

        #--- write buffer based on path
        if dirpath.startswith('s3'):
            s3utils.write_buff_to_s3path(file_path, buff)
        else:
            file_path = utils.path_sep_per_os(file_path)
            mode = 'wb' if type in ['binary','image'] else 'w'
            with open(file_path, mode) as file:
                file.write(buff)
        return file_path
        

    @staticmethod
    def save_data_list(data_list, dirname, name, format=None, type=None, subdir=None, s3flag=None):
        """ save a list of items in memory to disk, either locally of on s3
            returns list of pathnames
        """
        rootname, ext = os.path.splitext(name)
        pathlist = []
        for i, data_item in enumerate(data_list):
            if data_item is None:
                continue
            if type == 'image' and data_item.size == 0: 
                continue
            name = f"{rootname}-{(i + 1)}{ext}"
            pathlist.append(
                DB.save_data(data_item, dirname, name, format=format, type=type, subdir=subdir, s3flag=s3flag)
                )
        return pathlist


    @staticmethod
    def load_data_list(dirname, subdir=None, file_pat=None, s3flag=None):
        """ load list of items to memory from disk, either locally of on s3
        """
        
        file_list = DB.list_files_in_dirname_filtered(dirname=dirname, subdir=subdir, file_pat=file_pat, fullpaths=False, no_ext=False)
        
        data_list = []
        for filename in file_list:
            data_list.append(DB.load_data(dirname=dirname, name=filename, subdir=subdir, s3flag=s3flag))
            
        return data_list


    @staticmethod
    def upload_file_dirname(dirname, filename, local_dirname='EIFs', subdir=None):
        if filename is None:
            return

        s3dirpath = DB.dirpath_from_dirname(dirname=dirname, s3flag=True)
        s3filepath = s3dirpath + filename
        
        dirpath = DB.dirpath_from_dirname(dirname=dirname, s3flag=False, local_dirname='input_files')
        filepath = dirpath + filename
        
        s3utils.upload_filepath_to_s3path(filepath, s3filepath)
    

    #-------------------------------------------------
    
    @staticmethod
    def list_files_in_dirname_filtered(dirname, subdir=None, file_pat=None, fullpaths=False, no_ext=False, s3flag=None):
        """ list all files in dirname whether local or in s3 based on DB.MODE 
        
            use file_path=r'^[^~].*\.csv to exclude lock files.
        """
        
        dirpath = DB.dirpath_from_dirname(dirname, subdir=subdir, s3flag=s3flag)
        if dirpath.startswith('s3'):
            filelist = s3utils.list_files_in_s3dirpath(s3dirpath=dirpath, file_pat=file_pat, fullpaths=fullpaths)
        else:
            dirpath = utils.path_sep_per_os(dirpath)
            if file_pat:
                filelist = [n for n in os.listdir(dirpath) if bool(re.search(file_pat, n))]
            else:
                filelist = os.listdir(dirpath)
            if fullpaths:
                filelist = [os.path.join(dirpath, f) for f in filelist]
        if not fullpaths and no_ext:
            filelist = [os.path.splitext(f)[0] for f in filelist]
        return filelist


    @staticmethod
    def list_subdirs_with_filepat(dirname, file_pat=None, s3flag=None, subdir=None):
        """ used to list styles with templates produced.
        """
         
        dirpath = DB.dirpath_from_dirname(dirname, subdir=subdir, s3flag=s3flag)
        if dirpath.startswith('s3'):
            filelist = s3utils.list_files_in_prefix_s3(s3path=dirpath, file_pat=file_pat)
            subdir_list = [re.search(r'.*/([^/]*)/[^/]*$', m)[1] for m in filelist]
        else:
            dirpath = utils.path_sep_per_os(dirpath)
            
            matchlist = glob.glob(f"{dirpath}{os.sep}..{os.sep}{file_pat}")
            subdir_list = [re.search(fr'.*{os.sep}([^{os.sep}]*){os.sep}[^{os.sep}]*$', m)[1] for m in matchlist]
        return subdir_list

        
    @staticmethod
    def delete_dirname_files_filtered(dirname, subdir=None, file_pat=None, s3flag=None):
        filepaths = DB.list_files_in_dirname_filtered(dirname=dirname, subdir=subdir, file_pat=file_pat, fullpaths=True, s3flag=s3flag)
        if filepaths:
            if filepaths[0].startswith('s3'):
                s3utils.delete_s3paths(filepaths)
            else:
                utils.remove_local_files(filepaths)            

    @staticmethod
    def delete_s3_results(argsdict:dict, mode='all', subdir=None):
        job_folder_s3path = argsdict.get('job_folder_s3path')
        
        utils.sts(f"Deleting results from job_folder_s3path: '{job_folder_s3path}'", 3)
        if mode == 'all':
            for dirname in ['cmpcvr', 'marks', 'marks_chunks', 'cvr_chunks', 'lambda_tracker']:
                DB.delete_dirname_files_filtered(dirname=dirname, s3flag=True)
        else:
            DB.delete_dirname_files_filtered(dirname=subdir, s3flag=True)
        utils.sts("Finished")

    #-------------------------------------------------
    @staticmethod
    def check_if_exists(bucket, key):
        """ Checks if file/directory exists on S3 bucket
            DEPRECATED. USE DB.file_exists

        Args:
            bucket (str): name of the S3 bucket
            key (str): file path on S3 bucket

        Returns:
            boolean: True if key exists, False if key does not exist
        """
        s3_client = boto3.client('s3')
        try:
            s3_client.head_object(Bucket=bucket, Key=key)
            return True
        except ClientError:
            # Key not found
            return False

    @staticmethod
    def template_exists(style_num: str) -> bool:
        """ check styles directory for the template file and return true if it exists. """
        return DB.file_exists(
            file_name=f"{style_num}-template1.png", 
            dirname='styles', 
            subdir=style_num)
        # src_dirpath = DB.dirpath_from_dirname('styles') + f"{style_num}/"
        # return bool(os.path.exists(f"{src_dirpath}{style_num}-template1.png"))
        

    @staticmethod
    def file_exists(file_name, dirname, subdir=None, s3flag=False):
        dirpath = DB.dirpath_from_dirname('bif', subdir=subdir, s3flag=s3flag)

        if dirpath.startswith('s3'):
            s3path = posixpath.join(dirpath, file_name)
            return s3utils.does_s3path_exist(s3path)
        else:
            file_path = os.path.join(dirpath, file_name)
            return bool(os.path.isfile(file_path))
        

    @staticmethod
    def update_dict(dirname, name, field, value, subdir=None):
        d_dict = DB.load_data(dirname=dirname, name=name, subdir=subdir)
        d_dict[field] = value
        DB.save_data(data_item=d_dict, dirname=dirname, name=name, subdir=subdir)


    #-------------------------------------------------
    @staticmethod
    def get_bif_path(bif_name: str) -> str:
        dirpath = DB.dirpath_from_dirname('bif')
        return f"{dirpath}{bif_name}.csv"


    #-------------------------------------------------
    @staticmethod
    def download_entire_dirname(dirname: str, subdir=None, file_pat=None, local_dirname=None): #, keep_contents=False):
        """ given dirname on s3, download all files into dirname on local machine.
        Does not delete any files in local_dirname before download.
        """
        if local_dirname is None: local_dirname = dirname 
        
        utils.sts(f"Downloading files in '{dirname}' from s3 to '{local_dirname}' meeting file_pat:'{file_pat}'", 3)
        local_dirpath = DB.dirpath_from_dirname(local_dirname, subdir=subdir, s3flag=False)
        
        s3dirpath = DB.dirpath_from_dirname(dirname, subdir=subdir, s3flag=True)

        return s3utils.download_entire_s3dirpath_filtered(s3dirpath, local_dirpath, file_pat=file_pat)
   

    @staticmethod
    def combine_dirname_chunks(dirname, dest_name, dest_dirname=None, subdir=None, file_pat=r'chunk_\d+\.csv'):
        """ Combine chunks in a single dirname/subdir.
            after chunks are processed by lambdas, result chunks exist in dirname on s3.
            download all chunks to dirname folder and combine to dest_dirname
            uploads the combined file if on s3.
            NOTE: that this allows the chunks to remain on s3 so they can be used in later processing.
        """
        
        if dest_dirname is None: dest_dirname = dirname
        
        if not args.argsdict['use_s3_results']:
            # merge locally
            utils.merge_csv_dirname_local(dirname=dirname, dest_name=dest_name, file_pat=file_pat)
        else:   
            # download all the CSV files
            # make sure tmp is empty.
            tmp_dirpath = DB.dirpath_from_dirname('tmp', s3flag=False)
            shutil.rmtree(tmp_dirpath, ignore_errors=True)
            DB.download_entire_dirname(dirname=dirname, subdir=subdir, file_pat=file_pat, local_dirname='tmp')
            utils.merge_csv_dirname_local(dirname='tmp', subdir=subdir, dest_dirname=dirname, dest_name=dest_name, file_pat=file_pat)
            DB.upload_file_dirname(dirname=dest_dirname, filename=dest_name)    # note: no subdir used in combination.
           

    def combine_dirname_dfs(dirname, subdir=None, file_pat=None, s3flag=None):
        """
        Combine csv as dfs into a single df in memory.
        This function does not create any new files.
        """
        
        full_df = pd.DataFrame()
        names_list = DB.list_files_in_dirname_filtered(dirname=dirname, subdir=subdir, file_pat=file_pat, fullpaths=False, no_ext=False, s3flag=s3flag)
        for name in names_list:
            utils.sts(f"...combining dirname:'{dirname}' name:'{name}'", 3)
            this_df = DB.load_data(dirname=dirname, name=name, subdir=subdir, s3flag=s3flag, silent_error=False)
            full_df = full_df.append(this_df, ignore_index=True)
                
        utils.sts(f"Total of {len(full_df.index)} records in combined {dirname} df", 3)
        
        return full_df
        

    #---- alignment & style images ----------------------------
    @staticmethod
    def save_alignment_image(ballot_id, image, type='alignment'):
    
        DB.save_data(
            data_item   = image, 
            dirname     = 'styles', 
            name        = f"{ballot_id}-{type}.png", 
            subdir      = ballot_id
            )
    

    @staticmethod
    def save_style_image(style_num, image, type='genrois'):
        DB.save_data(
            data_item   = image, 
            dirname     = 'styles', 
            name        = f"{style_num}-{type}.png", 
            subdir      = style_num
            )
    
    #---- template images ----------------------------
    @staticmethod
    def save_template_images(style_num, images, file_type='template'):
    
        return DB.save_data_list(
            data_list   = images, 
            dirname     = 'styles', 
            name        = f"{style_num}-{file_type}.png", 
            format      = '.png', 
            subdir      = style_num
            )
    

    @staticmethod
    def load_template_images(style_num, img_type='template', s3flag = None):
    
    # filename for templates is like: 13102-template-1.png
    
        return DB.load_data_list(
            dirname     = 'styles', 
            subdir      = style_num, 
            file_pat    = fr'.*-{img_type}-\d+\.png', 
            s3flag      = s3flag,
            )
    

    @staticmethod
    def get_style_nums_with_templates(argsdict):
        """ return list of style_nums based on styles found in database.
        """
        dirpath = DB.dirpath_from_dirname('styles')
        if dirpath.startswith('s3'):
            return DB.get_style_nums_with_templates_s3(argsdict)
        else:
            return DB.get_style_nums_with_templates_local(argsdict)

   
    @staticmethod
    def get_style_nums_with_templates_local(argsdict):
        """ return list of style_nums based on styles found in database.
        """
        dirpath = DB.dirpath_from_dirname('styles', s3flag=False)
        try:
            filelist = os.listdir(dirpath)
        except FileNotFoundError:
            return []
        r = re.compile(r'^\d+$')
        style_nums = [x for x in filelist if r.match(x) and os.path.isdir(f'{dirpath}/{x}')]
        return style_nums


    @staticmethod
    def get_style_nums_with_templates_s3(argsdict):
        """Return list of style_nums based on styles found in S3 bucket.
        """
        logs.sts("Compiling style_nums with templates:", 3)
        from aws_lambda import s3utils
        
        s3dirpath = DB.dirpath_from_dirname('styles', s3flag=True)
        s3dict = s3utils.parse_s3path(s3dirpath)
        prefix = s3dict['prefix']

        style_nums = s3utils.list_files_in_prefix_s3(s3dirpath, file_pat=fr".*{prefix}([^/]+)/.*\-template\-1\.png$")

        logs.sts(f"Total of {len(style_nums)} style_nums found.", 3)
        return style_nums


    @staticmethod
    def copy_template(style_num: str, dst_dirname: str):
        """ copy templates and redlined PNG files from src_dirname to dst_dirname 
            this is for copying templates to assist
        """
        src_dirname = 'styles'
        
        for kind in ['template', 'redlined']:
            for p1 in [1, 2]:
                template_name = f"{style_num}-{kind}{p1}.png"
                utils.sts(f"Copying {template_name} from 'styles' to {dst_dirname}", 3)
                image = DB.load_data(
                    dirname=src_dirname, 
                    name=template_name, 
                    subdir=style_num,
                    silent_error=True)
                if not image: continue

                DB.save_data(data_item=image, 
                    dirname=dst_dirname, 
                    name=template_name, 
                    subdir=None)

                

    #---- template images ----------------------------
    @staticmethod
    def save_rois_images(style_num, images, s3flag=None):
    # styles/{style_num}/rois_parts/{style_num}-{part_name}.png 
    
        DB.save_data_list(
            data_list = images, 
            dirname = 'styles', 
            name    = f"{style_num}-rois_image.png", 
            subdir  = f"{style_num}/rois_parts", 
            s3flag  = s3flag)
    
    
    @staticmethod
    def save_one_image_area_dirname(dirname, style_num, idx, type_str, image, subdir=None, s3flag=None):
        # typical call.
        # DB.save_one_image_area_dirname(
            # dirname     = 'styles', 
            # subdir      = f"{style_dict['style_num']}/rois_parts", 
            # style_num   = style_dict['style_num'], idx='', 
            # type_str    = f"split_rois{p}", 
            # image       = cv_checkpoint_image)
    
        DB.save_data(
            data_item = image, 
            dirname = dirname, 
            name    = f"{style_num}-{idx}{type_str}.png", 
            subdir  = subdir, 
            s3flag  = s3flag)
    

    @staticmethod
    def load_rois_images(style_num, s3flag=None):
    
    # styles/{style_num}/rois_parts/{style_num}-{part_name}.png 
    
        return DB.load_data_list(
            dirname = 'styles', 
            subdir  = f"{style_num}/rois_parts", 
            file_pat= r'.*\-rois_image\d+\.png', 
            s3flag  = s3flag)
    

    @staticmethod
    def read_local_csv_to_df(file_path, user_format=False, dtype=None, silent_error=False):
        # please note: pandas does not skip comment lines that have commas in the comment.
        # skip_blank_lines=True in pd.read_csv does not work if na_filter=False
    
    
        file_path = utils.path_sep_per_os(file_path)
        if user_format:
            try:
                with open(file_path, mode='r') as fh:
                    buff = fh.read()
            except FileNotFoundError:
                if silent_error: 
                    return None
                else:
                    raise FileNotFoundError
            df = DB.buff_csv_to_df(buff, user_format=user_format)
                    
        else:   
            try:
                df = pd.read_csv(file_path, na_filter=False, index_col=False).replace(np.nan, '', regex=True)
            except FileNotFoundError:
                if silent_error: 
                    return None
                else:
                    raise FileNotFoundError
        return df

    @staticmethod
    def buff_csv_to_df(buff, user_format=False, dtype=None):
        if user_format:
            buff = utils.preprocess_csv_buff(buff)  # remove comments, blank lines
        sio = io.StringIO(buff)
            
        df  = pd.read_csv(sio, 
            na_filter=False, 
            index_col=False, 
            dtype=dtype,
            #sep=',', 
            #comment='#', 
            #skip_blank_lines=True
            ).replace(np.nan, '', regex=True)
        return df